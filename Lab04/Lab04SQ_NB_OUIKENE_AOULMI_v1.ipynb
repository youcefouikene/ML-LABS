{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7FtfhUkm3Mg"
      },
      "source": [
        "# 2CS-SIL2/SIQ2 Lab04. Naïve Bayes\n",
        "\n",
        "<p style='text-align: right;font-style: italic;'>Designed by: Mr. Abdelkrime Aries</p>\n",
        "\n",
        "In this lab, we will learn about Naive Bayes by testing 2 implementations:\n",
        "- Multinomial Naïve Bayes\n",
        "- Gaussian Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzqWTOPBm3Mo"
      },
      "source": [
        "**Team:**\n",
        "- **Member 01**: OUIKENE Youcef\n",
        "- **Member 02**: AOULMI Lina\n",
        "- **Group**: SIQ2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BEmutBPVm3Mq",
        "outputId": "c36e3efc-3016-4e5b-f3e7-affb408fa628"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import sys, timeit\n",
        "from typing          import Tuple, List, Type\n",
        "from collections.abc import Callable\n",
        "\n",
        "sys.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q07jh2KLm3Mu",
        "outputId": "95cc0b9b-8766-47bc-a0db-e74f24fd82a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.0.2', '2.2.2', '3.10.0')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy             as np\n",
        "import pandas            as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "np.__version__, pd.__version__, matplotlib.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vu8p03T-m3Mw",
        "outputId": "7f17200c-5379-4342-d180-4e54a79154a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.6.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import sklearn\n",
        "\n",
        "from sklearn.naive_bayes   import CategoricalNB\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.metrics       import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection         import train_test_split\n",
        "from sklearn.naive_bayes             import MultinomialNB, GaussianNB\n",
        "from sklearn.linear_model            import LogisticRegression\n",
        "from sklearn.tree                    import DecisionTreeClassifier\n",
        "from sklearn.metrics                 import precision_score, recall_score\n",
        "import timeit\n",
        "\n",
        "\n",
        "sklearn.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDsSjU6jm3Mx"
      },
      "source": [
        "## I. Algorithms implementation\n",
        "\n",
        "In this section, we will try to implement multinomial Naive Bayes.\n",
        "\n",
        "\n",
        "**>> Try to use \"numpy\" which will save a lot of time and effort**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVOykLH6m3My",
        "outputId": "887bbc5b-6051-4479-9494-d5bdcbea81be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Dataset play\n",
        "\n",
        "# outlook & temperature & humidity & windy\n",
        "Xplay = np.array([\n",
        "    ['sunny'   , 'hot' , 'high'  , 'no'],\n",
        "    ['sunny'   , 'hot' , 'high'  , 'yes'],\n",
        "    ['overcast', 'hot' , 'high'  , 'no'],\n",
        "    ['rainy'   , 'mild', 'high'  , 'no'],\n",
        "    ['rainy'   , 'cool', 'normal', 'no'],\n",
        "    ['rainy'   , 'cool', 'normal', 'yes'],\n",
        "    ['overcast', 'cool', 'normal', 'yes'],\n",
        "    ['sunny'   , 'mild', 'high'  , 'no'],\n",
        "    ['sunny'   , 'cool', 'normal', 'no'],\n",
        "    ['rainy'   , 'mild', 'normal', 'no'],\n",
        "    ['sunny'   , 'mild', 'normal', 'yes'],\n",
        "    ['overcast', 'mild', 'high'  , 'yes'],\n",
        "    ['overcast', 'hot' , 'normal', 'no'],\n",
        "    ['rainy'   , 'mild', 'high'  , 'yes']\n",
        "])\n",
        "\n",
        "Yplay = np.array([\n",
        "    'no',\n",
        "    'no',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'no',\n",
        "    'yes',\n",
        "    'no',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'no'\n",
        "])\n",
        "\n",
        "len(Xplay), len(Yplay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq-qPnblm3Mz",
        "outputId": "6d6972c2-a627-4d63-d787-08338d388eb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# height & weight & footsize & person\n",
        "Xperson = np.array([\n",
        "    [182., 81.6, 30.],\n",
        "    [180., 86.2, 28.],\n",
        "    [170., 77.1, 30.],\n",
        "    [180., 74.8, 25.],\n",
        "    [152., 45.4, 15.],\n",
        "    [168., 68.0, 20.],\n",
        "    [165., 59.0, 18.],\n",
        "    [175., 68.0, 23.]\n",
        "])\n",
        "\n",
        "Yperson = np.array([\n",
        "    'male', 'male', 'male', 'male',\n",
        "    'female', 'female', 'female', 'female'\n",
        "])\n",
        "\n",
        "len(Xperson), len(Yperson)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzA4kyNfm3M0"
      },
      "source": [
        "### I.1. Prior statistics\n",
        "\n",
        "Given an output list $Y[M]$, the probability of each class $c$ is estimated as:\n",
        "$$p(c) = \\frac{\\#(Y = c)}{|Y|}$$\n",
        "\n",
        "In here, we want to store the frequencies of different classes.\n",
        "Our function must return two lists:\n",
        "- One containing the names of unique classes.\n",
        "- Another containing their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSuRAqKxm3M0",
        "outputId": "01960065-7418-45ea-99cf-0a44c6df4ef9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array(['no', 'yes'], dtype='<U3'), array([5, 9])),\n",
              " (array(['female', 'male'], dtype='<U6'), array([4, 4])))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# TODO: Prior statistics\n",
        "def fit_prior(Y: 'np.ndarray[M](str)') -> Tuple['np.ndarray[K](str)', 'np.ndarray[K](int)']:\n",
        "    return np.unique(Y,return_counts=True)\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# ((array(['no', 'yes'], dtype='<U3'), array([5, 9])),\n",
        "#  (array(['female', 'male'], dtype='<U6'), array([4, 4])))\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "fit_prior(Yplay), fit_prior(Yperson)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kjF6Axpm3M1"
      },
      "source": [
        "### I.2. Multinomial Law\n",
        "\n",
        "In this section, we will implement multinomial naive Bayes from scratch using Numpy.\n",
        "\n",
        "#### I.2.1. Multinomial Likelihood statistics\n",
        "\n",
        "Given:\n",
        "- $A$: a categorical feature\n",
        "- $Y$: the ouput\n",
        "- $C$: the classes\n",
        "\n",
        "The function takes as argument $A, Y, C$ previously described.\n",
        "It must return:\n",
        "- $V$: unique values of this feature (feature's categories)\n",
        "- $S[|C|, |V|]$: a matrix containing count $\\#(Y = c \\wedge A = v),\\, \\forall c \\in C, \\forall v \\in A$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT2mSaIDm3M1",
        "outputId": "b7b71905-e3d2-407f-bff7-2374472e8a8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
              "  array([[0, 2, 3],\n",
              "         [4, 3, 2]])),\n",
              " (array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
              "  array([[1, 2, 2],\n",
              "         [3, 2, 4]])))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# TODO: Multinomial Likelihood statistics\n",
        "def fit_multinomial_likelihood(A: 'np.ndarray[M](str)',\n",
        "                               Y: 'np.ndarray[M](str)',\n",
        "                               C: 'np.ndarray[C](str)'\n",
        "                               ) -> Tuple['np.ndarray[V](str)', 'np.ndarray[C, V](int)']:\n",
        "    V = np.unique(A)\n",
        "    S = np.zeros([len(C),len(V)],dtype=int)\n",
        "    for i, c in enumerate(C):\n",
        "        for j ,v in enumerate(V):\n",
        "            S[i,j]=np.sum((A==v)&(Y==c))\n",
        "    return V,S\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# ((array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
        "#   array([[0, 2, 3],\n",
        "#          [4, 3, 2]])),\n",
        "#  (array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
        "#   array([[1, 2, 2],\n",
        "#          [3, 2, 4]])))\n",
        "#---------------------------------------------------------------------\n",
        "C_t = np.array(['no', 'yes'])\n",
        "fit_multinomial_likelihood(Xplay[:, 0], Yplay, C_t), fit_multinomial_likelihood(Xplay[:, 1], Yplay, C_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUeH7W09m3M2"
      },
      "source": [
        "#### I.2.2. Multinomial Likelihood training\n",
        "\n",
        "**Nothing to code here, although you have to know how it functions for next use**\n",
        "\n",
        "This function aims to generate parameters $\\theta$.\n",
        "In our case, paramters are diffrent from those of *logistic regrssion*.\n",
        "They are a dictionary (map) with two entries:\n",
        "- \"prior\": a dictionary having \"vocab\" a list of values and \"freq\" a list of their respective frequencies.\n",
        "- \"likelihood\": a list of dictionaries representing statistics of each feature (the same order of $X$ features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgBoKrHbm3M3",
        "outputId": "3cff302d-5077-4534-ed85-632bc3372251"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prior': {'vocab': array(['no', 'yes'], dtype='<U3'), 'freq': array([5, 9])},\n",
              " 'likelihood': [{'vocab': array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
              "   'freq': array([[0, 2, 3],\n",
              "          [4, 3, 2]])},\n",
              "  {'vocab': array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
              "   'freq': array([[1, 2, 2],\n",
              "          [3, 2, 4]])},\n",
              "  {'vocab': array(['high', 'normal'], dtype='<U8'),\n",
              "   'freq': array([[4, 1],\n",
              "          [3, 6]])},\n",
              "  {'vocab': array(['no', 'yes'], dtype='<U8'),\n",
              "   'freq': array([[2, 3],\n",
              "          [6, 3]])}]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def fit_multinomial_NB(X: 'np.ndarray[M, N](str)',\n",
        "                       Y: 'np.ndarray[M](str)'\n",
        "                       ) -> object:\n",
        "\n",
        "    Theta   = {'prior': {}, 'likelihood': []}\n",
        "\n",
        "    Theta['prior']['vocab'], Theta['prior']['freq'] = fit_prior(Y)\n",
        "\n",
        "    for j in range(X.shape[1]):\n",
        "        likelihood = {}\n",
        "        likelihood['vocab'], likelihood['freq'] = fit_multinomial_likelihood(X[:, j], Y, Theta['prior']['vocab'])\n",
        "        Theta['likelihood'].append(likelihood)\n",
        "\n",
        "    return Theta\n",
        "\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# {'prior': {'vocab': array(['no', 'yes'], dtype='<U3'), 'freq': array([5, 9])},\n",
        "#  'likelihood': [{'vocab': array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
        "#    'freq': array([[0, 2, 3],\n",
        "#           [4, 3, 2]])},\n",
        "#   {'vocab': array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
        "#    'freq': array([[1, 2, 2],\n",
        "#           [3, 2, 4]])},\n",
        "#   {'vocab': array(['high', 'normal'], dtype='<U8'),\n",
        "#    'freq': array([[4, 1],\n",
        "#           [3, 6]])},\n",
        "#   {'vocab': array(['no', 'yes'], dtype='<U8'),\n",
        "#    'freq': array([[2, 3],\n",
        "#           [6, 3]])}]}\n",
        "#---------------------------------------------------------------------\n",
        "Theta_play = fit_multinomial_NB(Xplay, Yplay)\n",
        "\n",
        "Theta_play"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73ay52Ebm3M4"
      },
      "source": [
        "#### I.2.3. Multinomial Likelihood prediction\n",
        "\n",
        "Given:\n",
        "- $A$: a categorical feature\n",
        "- $V$: unique values of this feature (feature's categories)\n",
        "- $Y$: the ouput\n",
        "- $C$: the classes\n",
        "- $\\alpha$: smoothing factor\n",
        "\n",
        "Log likelihood is calculated as:\n",
        "$$ \\log p(A=v|Y=c) = \\log(\\#(Y = k \\wedge A = v) + \\alpha) - \\log(\\#(y = k) + \\alpha * |V|)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iad7LlKUm3M4",
        "outputId": "630e1a89-3fd7-4103-cc9f-36b685b242c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.int64(1), -1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# You can use this function in the next implimentation\n",
        "# It takes a list of unique values V and a given value v\n",
        "# It returns the position of v in V\n",
        "# If v does not exist in V, it rturns -1\n",
        "def find_idx(V: np.ndarray, v: str) -> int:\n",
        "    k = np.argwhere(V == v).flatten()\n",
        "    if len(k):\n",
        "        return k[0]\n",
        "    return -1\n",
        "\n",
        "V_t = np.array(['One', 'Two', 'Three'])\n",
        "find_idx(V_t, 'Two'), find_idx(V_t, 'Four')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF_Uk8Lhm3M5",
        "outputId": "f31e094f-2e67-4395-d95a-18f10c7dfbe1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.91629073, -1.09861229]), array([-2.07944154, -2.48490665]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# TODO: Multinomial Likelihood prediction\n",
        "def predict_multinomial_NB1(v: str,\n",
        "                            j: int,\n",
        "                            Theta: object,\n",
        "                            alpha: float = 0.\n",
        "                            ) -> 'np.ndarray[C](float)':\n",
        "\n",
        "    k = find_idx(Theta['likelihood'][j]['vocab'], v)\n",
        "\n",
        "    if (k == -1):\n",
        "\n",
        "        return np.log(alpha) - np.log(Theta['prior']['freq'] + alpha * len(Theta['likelihood'][j]['vocab']))\n",
        "\n",
        "    else:\n",
        "\n",
        "        return np.log(Theta['likelihood'][j]['freq'][:, k] + alpha) - np.log(Theta['prior']['freq'] + alpha * len(Theta['likelihood'][j]['vocab']))\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# (array([-0.91629073, -1.09861229]), array([-2.07944154, -2.48490665]))\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "X_t = np.array([\n",
        "    ['rainy', 'cool', 'normal', 'yes'],\n",
        "    ['snowy', 'cool', 'normal', 'yes'],\n",
        "    ['sunny', 'hot' , 'normal', 'no']\n",
        "])\n",
        "\n",
        "predict_multinomial_NB1('rainy', 0, Theta_play, alpha=0.), \\\n",
        "    predict_multinomial_NB1('snowy', 0, Theta_play, alpha=1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is3XN-gtm3M6"
      },
      "source": [
        "### I.3. Normal (Gaussian) Law\n",
        "\n",
        "In this section, we will implement gaussian naive Bayes from scratch using Numpy.\n",
        "\n",
        "#### I.3.1. Gaussian Likelihood statistics\n",
        "\n",
        "Given:\n",
        "- $A$: a categorical feature\n",
        "- $Y$: the ouput\n",
        "- $C$: the classes\n",
        "\n",
        "The function takes as argument $A, Y, C$ previously described.\n",
        "It must return $S[|C|, 2, N]$; a tensor having these dimensions:\n",
        "- first dimension: each element represents one class's statistics\n",
        "- second dimension: 1st element represents means; 2ns element represents variances\n",
        "- third dimension: each element represents mean/variance of the respective feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfGoRseem3M6",
        "outputId": "f8d50ab7-2f42-4759-91dc-7d19aceba335"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[165.        ,  60.1       ,  19.        ],\n",
              "        [ 92.66666667, 114.04      ,  11.33333333]],\n",
              "\n",
              "       [[178.        ,  79.925     ,  28.25      ],\n",
              "        [ 29.33333333,  25.47583333,   5.58333333]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# TODO: Gaussian Likelihood statistics\n",
        "def fit_gaussian_likelihood(X: 'np.ndarray[M](float)',\n",
        "                            Y: 'np.ndarray[M](str)',\n",
        "                            C: 'np.ndarray[C](str)'\n",
        "                            ) -> Tuple['np.ndarray[C, 2, N](float)']:\n",
        "    XY = np.column_stack([X,Y])\n",
        "    S = np.zeros([len(C),2,X.shape[1]])\n",
        "    for i in range(len(C)):\n",
        "        for j in range(X.shape[1]):\n",
        "            # taking either male occurences or female occurences\n",
        "            cX = XY[XY[:, -1] == C[i], j].astype(float)\n",
        "            S[i,0,j]=np.mean(cX)\n",
        "            S[i,1,j]=np.var(cX,ddof=1) # divide by n-1 and not n, so use ddof = 1\n",
        "    return S\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# array([[[165.        ,  60.1       ,  19.        ],\n",
        "#         [ 92.66666667, 114.04      ,  11.33333333]],\n",
        "\n",
        "#        [[178.        ,  79.925     ,  28.25      ],\n",
        "#         [ 29.33333333,  25.47583333,   5.58333333]]])\n",
        "#---------------------------------------------------------------------\n",
        "C_t = np.array(['female', 'male'])\n",
        "fit_gaussian_likelihood(Xperson, Yperson, C_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm2di7gjm3M7"
      },
      "source": [
        "#### I.3.2. Gaussian Likelihood training\n",
        "\n",
        "**Nothing to code here, although you have to know how it functions for next use**\n",
        "\n",
        "This function aims to generate parameters $\\theta$.\n",
        "In our case, paramters are diffrent from those of *logistic regrssion*.\n",
        "They are a dictionary (map) with two entries:\n",
        "- \"prior\": a dictionary having \"vocab\" a list of values and \"freq\" a list of their respective frequencies.\n",
        "- \"likelihood\": a tensor of shape $[|C|, 2, N]$ containing likelihood statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG8lKfvfm3M7",
        "outputId": "59db844c-27c9-4bb0-8568-0bbb8da950b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prior': {'vocab': array(['female', 'male'], dtype='<U6'),\n",
              "  'freq': array([4, 4])},\n",
              " 'likelihood': array([[[165.        ,  60.1       ,  19.        ],\n",
              "         [ 92.66666667, 114.04      ,  11.33333333]],\n",
              " \n",
              "        [[178.        ,  79.925     ,  28.25      ],\n",
              "         [ 29.33333333,  25.47583333,   5.58333333]]])}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "def fit_gaussian_NB(X: 'np.ndarray[M, N](str)',\n",
        "                    Y: 'np.ndarray[M](str)'\n",
        "                    ) -> object:\n",
        "\n",
        "    Theta   = {'prior': {}, 'likelihood': []}\n",
        "\n",
        "    Theta['prior']['vocab'], Theta['prior']['freq'] = fit_prior(Y)\n",
        "    Theta['likelihood'] = fit_gaussian_likelihood(X, Y, Theta['prior']['vocab'])\n",
        "\n",
        "    return Theta\n",
        "\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# {'prior': {'vocab': array(['female', 'male'], dtype='<U6'),\n",
        "#   'freq': array([4, 4])},\n",
        "#  'likelihood': array([[[165.        ,  60.1       ,  19.        ],\n",
        "#          [ 92.66666667, 114.04      ,  11.33333333]],\n",
        "\n",
        "#         [[178.        ,  79.925     ,  28.25      ],\n",
        "#          [ 29.33333333,  25.47583333,   5.58333333]]])}\n",
        "#---------------------------------------------------------------------\n",
        "Theta_person = fit_gaussian_NB(Xperson, Yperson)\n",
        "\n",
        "Theta_person"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsVso5b7m3M8"
      },
      "source": [
        "#### I.2.4. Gaussian Likelihood prediction\n",
        "\n",
        "Given:\n",
        "- $A$: a numerical feature\n",
        "- $\\mu_{Ac}$: mean of values of feature $A$ having $c$ as class\n",
        "- $\\sigma_{Ac}$: variance of values of feature $A$ having $c$ as class\n",
        "- $Y$: the output\n",
        "- $C$: the classes\n",
        "\n",
        "Log likelihood is calculated as:\n",
        "$$ \\log p(A=v|Y=c) = \\frac{-(v-\\mu_{Ac})^2}{2 \\sigma_{Ac}^2} - \\log(\\sqrt{2\\pi \\sigma_{Ac}^2})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVxc_qBHm3M8",
        "outputId": "d640aa97-9254-4840-e15b-e99d1293db35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-4.93164438, -3.03443716]), array([0.00721463, 0.04810173]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# TODO: Gaussian Likelihood prediction\n",
        "def predict_gaussian_NB1(v: str,\n",
        "                         j: int,\n",
        "                         Theta: object,\n",
        "                         alpha: float = 0. # this is just added for compatibility\n",
        "                         ) -> 'np.ndarray[C](float)':\n",
        "\n",
        "    return - (np.power(v - Theta['likelihood'][:, 0, j], 2)) / (2 * Theta['likelihood'][:, 1, j]) - np.log(np.sqrt(2 * np.pi * Theta['likelihood'][:, 1, j]))\n",
        "\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# (array([-4.93164438, -3.03443716]), array([0.00721463, 0.04810173]))\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "pp = predict_gaussian_NB1(183, 0, Theta_person)\n",
        "\n",
        "pp, np.exp(pp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggqs2NQsm3M9"
      },
      "source": [
        "### I.4. Final prediction\n",
        "\n",
        "Our goal is to calculate approximate log probabilities of all classes given a sample:\n",
        "$$\\log P(y=c_k | \\overrightarrow{x} = \\overrightarrow{f})  \\approx \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j = x_j|y=c_k)$$\n",
        "\n",
        "This function takes:\n",
        "- $X^{(i)}$ one sample with $N$ features\n",
        "- $\\theta$ parameters (either those of multinomial or gaussian)\n",
        "- $pred_{fct}$ a function to predict one feauture (either multinomial or gaussian)\n",
        "- add_prior: if True, add prior probability\n",
        "- $\\alpha$ smoothing factor (passing it to gaussian function will do nothing)\n",
        "\n",
        "It must return a vector of probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzm0cPUSm3M9",
        "outputId": "c68182e9-c419-4300-a2f0-3d09ac81e015"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-3.59617006, -4.95406494]),\n",
              " array([-2.56655064, -4.51223219]),\n",
              " array([-2.85774653, -4.23617476]),\n",
              " array([-10.401093  , -22.03977023]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# TODO: Final prediction\n",
        "def predict_NB1(Xi    : 'np.ndarray[N]',\n",
        "                Theta: object,\n",
        "                pred_fct: Callable,\n",
        "                add_prior: bool  = True,\n",
        "                alpha: float = 1.0\n",
        "                ) -> 'np.ndarray[C](float)':\n",
        "\n",
        "    yFreq = Theta['prior']['freq']\n",
        "    yFreq = yFreq/sum(yFreq)\n",
        "    priorLog = np.log(yFreq)\n",
        "\n",
        "    labels = Theta['prior']['vocab']\n",
        "    logSum = np.zeros([len(labels)])\n",
        "    for i in range(len(Xi)):\n",
        "        logSum +=pred_fct(Xi[i],i,Theta,alpha)\n",
        "\n",
        "    return logSum+priorLog*add_prior\n",
        "\n",
        "# Changed like in classroom\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# (array([-3.59617006, -4.95406494]),\n",
        "# array([-2.56655064, -4.51223219]),\n",
        "# array([-2.85774653, -4.23617476]),\n",
        "# array([-10.401093 , -22.03977023]))\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "X_t1 = np.array(['sunny', 'hot' , 'high', 'no'])\n",
        "X_t2 = np.array([183., 59., 20.])\n",
        "\n",
        "predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=True, alpha=0.0), \\\n",
        "predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=False, alpha=0.0), \\\n",
        "predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=False, alpha=1.0), \\\n",
        "predict_NB1(X_t2, Theta_person, predict_gaussian_NB1, add_prior=False),"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54bWk6hbm3M9"
      },
      "source": [
        "### I.5. Final product\n",
        "\n",
        "**>> Nothing to code here**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JMm03QJm3M-",
        "outputId": "78fbccdd-0380-432b-df96-2aa73be350f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['yes', 'yes', 'no'], dtype='<U3'),\n",
              " array([[-5.20912179, -4.10264337],\n",
              "        [-6.30773408, -5.48893773],\n",
              "        [-3.88736595, -4.67800751]]),\n",
              " array(['female', 'male'], dtype='<U6'),\n",
              " array([[-11.09424018, -22.73291741],\n",
              "        [-15.27968966, -12.41764665]]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "class NaiveBayes(object):\n",
        "\n",
        "    def __init__(self, multinomial=True):\n",
        "        if multinomial:\n",
        "            self.train = fit_multinomial_NB\n",
        "            self.pred = predict_multinomial_NB1\n",
        "        else:\n",
        "            self.train = fit_gaussian_NB\n",
        "            self.pred = predict_gaussian_NB1\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        self.Theta = self.train(X, Y)\n",
        "\n",
        "    def predict(self, X, add_prior=True, prob=False, alpha=0.):\n",
        "        Y_pred = []\n",
        "        for i in range(len(X)):\n",
        "            Y_pred.append(predict_NB1(\n",
        "                X[i,:], self.Theta, self.pred, add_prior=add_prior, alpha=alpha\n",
        "                ))\n",
        "\n",
        "        Y_pred = np.array(Y_pred)\n",
        "\n",
        "        if prob:\n",
        "            return Y_pred\n",
        "\n",
        "        return np.choose(np.argmax(Y_pred, axis=1), self.Theta['prior']['vocab'])\n",
        "\n",
        "# Changed like in classroom\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# (array(['yes', 'yes', 'no'], dtype='<U3'),\n",
        "# array([[-5.20912179, -4.10264337],\n",
        "# [-6.30773408, -5.48893773],\n",
        "# [-3.88736595, -4.67800751]]),\n",
        "# array(['female', 'male'], dtype='<U6'),\n",
        "# array([[-11.09424018, -22.73291741],\n",
        "# [-15.27968966, -12.41764665]]))\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "multinomial_nb = NaiveBayes()\n",
        "multinomial_nb.fit(Xplay, Yplay)\n",
        "\n",
        "gaussian_nb = NaiveBayes(multinomial=False)\n",
        "gaussian_nb.fit(Xperson, Yperson)\n",
        "\n",
        "X_t1 = np.array([\n",
        "    ['rainy', 'cool', 'normal', 'yes'],\n",
        "    ['snowy', 'cool', 'normal', 'yes'],\n",
        "    ['sunny', 'hot' , 'high', 'no']\n",
        "])\n",
        "\n",
        "X_t2 = np.array([\n",
        "    [183., 59., 20.],\n",
        "    [175., 65., 30.]\n",
        "])\n",
        "\n",
        "\n",
        "multinomial_nb.predict(X_t1, alpha=1.), \\\n",
        "    multinomial_nb.predict(X_t1, alpha=1., prob=True), \\\n",
        "    gaussian_nb.predict(X_t2), \\\n",
        "    gaussian_nb.predict(X_t2, prob=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJLxDnmDm3M_"
      },
      "source": [
        "## II. Application and Analysis\n",
        "\n",
        "In this section, we will test different concepts by running an experiment, formulating a hypothesis and trying to justify it.\n",
        "\n",
        "### II.1. Prior probability\n",
        "\n",
        "We want to test the effect of prior probability.\n",
        "To do this, we trained two models:\n",
        "1. With prior probability\n",
        "1. Without prior probability (It considers a uniform distribution of classes)\n",
        "\n",
        "To test whether the models have adapted well to the training dataset, we will test them on the same dataset and calculate the classification ratio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3YiokSem3NA",
        "outputId": "399f04f3-2c07-4ace-ee03-bc4d9e72a2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Considring prior probability\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       1.00      0.80      0.89         5\n",
            "         yes       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.93        14\n",
            "   macro avg       0.95      0.90      0.92        14\n",
            "weighted avg       0.94      0.93      0.93        14\n",
            "\n",
            "No prior probability\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.67      0.80      0.73         5\n",
            "         yes       0.88      0.78      0.82         9\n",
            "\n",
            "    accuracy                           0.79        14\n",
            "   macro avg       0.77      0.79      0.78        14\n",
            "weighted avg       0.80      0.79      0.79        14\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nb_withPrior     = CategoricalNB(alpha=1.0, fit_prior=True )\n",
        "nb_noPrior       = CategoricalNB(alpha=1.0, fit_prior=False)\n",
        "\n",
        "enc         = OrdinalEncoder()\n",
        "Xplay_tf    = enc.fit_transform(Xplay)\n",
        "nb_withPrior.fit(Xplay_tf, Yplay)\n",
        "nb_noPrior.fit(Xplay_tf, Yplay)\n",
        "\n",
        "Ypred_withPrior = nb_withPrior.predict(Xplay_tf)\n",
        "Ypred_noPrior = nb_noPrior.predict(Xplay_tf)\n",
        "\n",
        "\n",
        "print( 'Considring prior probability'  )\n",
        "print(classification_report(Yplay, Ypred_withPrior))\n",
        "\n",
        "print( 'No prior probability'  )\n",
        "print(classification_report(Yplay, Ypred_noPrior))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIjiji4Zm3NC"
      },
      "source": [
        "**TODO: Analyze the results**\n",
        "\n",
        "1. What do you notice, indicating if prior probability is useful in this case?\n",
        "1. How does this probability affect the outcome?\n",
        "1. When are we sure that using this probability is unnecessary?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "1. We observe that incorporating the prior probability leads to higher overall accuracy, precision, recall, and F1-score compared to when it is not considered. This suggests that, in this case, taking the prior probability into account improves performance and is therefore beneficial.\n",
        "\n",
        "1. This probability affects the outcome by increasing the probability of the most frequent class. We said that the output class will be the class that maximizes the following quantity: $p(y = k) \\prod_{j=1}^N P(x_j | y = k), \\, k \\in \\{1, \\dots, L\\}$. As a result, the most frequent class in our training set will have more chances of maximizing this quantity, making it more likely to be assigned to a new input.\n",
        "\n",
        "1. Using the prior probability becomes unnecessary when the class distribution is uniform, meaning each class appears equally in the training data. As mentioned, the predicted class is the one that maximizes $p(y = k) \\prod_{j=1}^N P(x_j | y = k)$.Since all classes have the same frequency, i.e the quantity $p(y = k)$ is equal across all classes. Maximising $p(y = k) \\prod_{j=1}^N P(x_j | y = k), \\, k \\in \\{1, \\dots, L\\}$ will be the same as maximizing $\\prod_{j=1}^N P(x_j | y = k), \\, k \\in \\{1, \\dots, L\\}$. Therefore, there's no need for prior probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BJ5EuEZm3NC"
      },
      "source": [
        "### II.2. Smoothing\n",
        "\n",
        "We want to test the Lidstone smoothing's effect.\n",
        "To do this, we trained three models:\n",
        "1. alpha = 1 (Laplace smoothing)\n",
        "1. alpha = 0.5\n",
        "1. alpha = 0 (without smoothing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPE0HXmDm3ND",
        "outputId": "e30b384a-f7d5-4100-8fa7-97025d6c2e5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alpha = 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       1.00      0.80      0.89         5\n",
            "         yes       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.93        14\n",
            "   macro avg       0.95      0.90      0.92        14\n",
            "weighted avg       0.94      0.93      0.93        14\n",
            "\n",
            "Alpha = 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       1.00      0.80      0.89         5\n",
            "         yes       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.93        14\n",
            "   macro avg       0.95      0.90      0.92        14\n",
            "weighted avg       0.94      0.93      0.93        14\n",
            "\n",
            "Alpha = 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       1.00      0.80      0.89         5\n",
            "         yes       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.93        14\n",
            "   macro avg       0.95      0.90      0.92        14\n",
            "weighted avg       0.94      0.93      0.93        14\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/env/ml/lib/python3.10/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
            "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n"
          ]
        }
      ],
      "source": [
        "NBC_10 = CategoricalNB(alpha = 1.0 )\n",
        "NBC_05 = CategoricalNB(alpha = 0.5 )\n",
        "NBC_00 = CategoricalNB(alpha = 0.0 )\n",
        "\n",
        "NBC_10.fit( Xplay_tf,   Yplay )\n",
        "NBC_05.fit( Xplay_tf,   Yplay )\n",
        "NBC_00.fit( Xplay_tf,   Yplay )\n",
        "\n",
        "Y_10   = NBC_10.predict(Xplay_tf)\n",
        "Y_05   = NBC_05.predict(Xplay_tf)\n",
        "Y_00   = NBC_00.predict(Xplay_tf)\n",
        "\n",
        "\n",
        "print(                'Alpha = 1.0'                        )\n",
        "print(classification_report(Yplay, Y_10, zero_division=0.0))\n",
        "\n",
        "print(                'Alpha = 0.5'                        )\n",
        "print(classification_report(Yplay, Y_05, zero_division=0.0))\n",
        "\n",
        "print(                'Alpha = 0.0'                        )\n",
        "print(classification_report(Yplay, Y_00, zero_division=0.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVC6qulzm3NE"
      },
      "source": [
        "**TODO: Analyze the results**\n",
        "\n",
        "1. What do you notice, indicating if smoothing affects performance in this case?\n",
        "1. Based on the past answeer, Why?\n",
        "1. Why do we get a \"RuntimeWarning: divide by zero\" error?\n",
        "1. What is the benefit of smoothing (generally; not just for this case)?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "1. No, smoothing doesn’t make a difference here. Even with different alpha values (1, 0.5, 0), the model produces identical precision, recall, and F1-scores across classes. This consistency shows that smoothing isn’t needed for the model to reach accurate predictions, suggesting the data has no major gaps or imbalances that smoothing would typically address.\n",
        "\n",
        "2. Smoothing is generally applied when there is a feature $x_j$ that takes a value $v$ in the test dataset that was never previously seen in the training dataset. However, since we tested the performance of three models on a test dataset which is identical to the training dataset, there isn't a case where the feature $x_j$ takes a value $v$ which wasn't already seen by the model in the training dataset, so the smoothing factor $\\alpha$ wasn't used when estimating the probabilities, so the predicted output by the three models would be in this case very much identical, and thus the three models would have the same performance despite having different values for $\\alpha$. also this inequality : $P(Y=\\text{YES}).P(X|Y=\\text{YES})>P(Y=\\text{NO}).P(X|Y=\\text{NO})$ will not change after adding smoothing, because smoothing adjusts both likelihoods proporitonally (smoothing does not drastically alter which probability is larger since smoothing applies the same effect to both probabilities), thus, the predicted class would still likely be \"YES\"\n",
        "\n",
        "3. When predicting a new input, we said that we find the class $k$ that maximizes $P(y = k) \\prod_{j=1}^N P(x_j | y = k), \\, k \\in \\{1, \\dots, L\\}$, which is the same as finding the class $k$ which maximizes $\\log P(y=c_k) + \\sum_{j=1}^N log P(x_j | y = k), \\, k \\in \\{1, \\dots, L\\} = \\log P(y=c_k) + \\sum_{j=1}^N \\log(\\#(Y = k \\wedge x_j= v) + \\alpha) - \\log(\\#(y = k) + \\alpha * |V|), \\, k \\in \\{1, \\dots, L\\} $ Sometimes, the quantity $\\#(Y = k \\wedge x_j= v)$ might be 0 (there's no instance where the feature $x_j$ takes the value $v$ and it belongs to the class $k$), and if we have set the smoothing rate to be zero ($\\alpha= 0$), we could calculate a log(0), which is impossible, explains why sklearn raised an error.\n",
        "\n",
        "4. Smoothing is typically used in Naive Bayes classification to handle the problem of zero probabilities, which can occur when a particular feature value is not present in the training data for a given class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBsrarXOm3NE"
      },
      "source": [
        "### II.3. Naive Bayes performance\n",
        "\n",
        "Naive Bayes is known to generate powerful models when it comes to classifying textual documents.\n",
        "We want to test this proposition using spam detection over [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) dataset.\n",
        "\n",
        "Each message is represented using term frequency (TF), where a word is considered as a feature.\n",
        "In this case, a message is represented by a vector of frequencies (how many times each word appeared in the message).\n",
        "We want to compare these models:\n",
        "1. Multinomial Naive Bayes (MNB)\n",
        "1. Gaussian Naive Bayes (GNB)\n",
        "1. Logistic Regression (LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I13ZgD0m3NF",
        "outputId": "e02819be-4bf7-4a85-ca04-7995925c0e1e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text class\n",
              "0  Go until jurong point, crazy.. Available only ...   ham\n",
              "1                      Ok lar... Joking wif u oni...   ham\n",
              "2  Free entry in 2 a wkly comp to win FA Cup fina...  spam\n",
              "3  U dun say so early hor... U c already then say...   ham\n",
              "4  Nah I don't think he goes to usf, he lives aro...   ham"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reading the dataset\n",
        "messages = pd.read_csv('data/spam.csv', encoding='latin-1')\n",
        "# renaming features: text and class\n",
        "messages = messages.rename(columns={'v1': 'class', 'v2': 'text'})\n",
        "# keeping only these two features\n",
        "messages = messages.filter(['text', 'class'])\n",
        "\n",
        "messages.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4KgkkRSm3NG",
        "outputId": "81e9d4b4-e6a5-4fe9-dc2c-c2011628c04e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Algorithm</th>\n",
              "      <th>Train time</th>\n",
              "      <th>Test time</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Multinomial Naive Bayes (MNB)</td>\n",
              "      <td>0.288778</td>\n",
              "      <td>0.021871</td>\n",
              "      <td>0.987179</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Gaussian Naive Bayes  (GNB)</td>\n",
              "      <td>0.420813</td>\n",
              "      <td>0.095978</td>\n",
              "      <td>0.616667</td>\n",
              "      <td>0.891566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logistic Regression (LR)</td>\n",
              "      <td>0.551115</td>\n",
              "      <td>0.022939</td>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.855422</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Algorithm  Train time  Test time  Precision    Recall\n",
              "0  Multinomial Naive Bayes (MNB)    0.288778   0.021871   0.987179  0.927711\n",
              "1    Gaussian Naive Bayes  (GNB)    0.420813   0.095978   0.616667  0.891566\n",
              "2       Logistic Regression (LR)    0.551115   0.022939   0.986111  0.855422"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models = [\n",
        "    MultinomialNB(),\n",
        "    GaussianNB(),\n",
        "    LogisticRegression(solver='lbfgs'),\n",
        "    #solver=sag is slower; so I chose the fastest\n",
        "]\n",
        "\n",
        "algos = [\n",
        "    'Multinomial Naive Bayes (MNB)',\n",
        "    'Gaussian Naive Bayes  (GNB)',\n",
        "    'Logistic Regression (LR)',\n",
        "]\n",
        "\n",
        "perf = {\n",
        "    'train_time': [],\n",
        "    'test_time' : [],\n",
        "    'recall'    : [],\n",
        "    'precision' : []\n",
        "}\n",
        "\n",
        "\n",
        "msg_train, msg_test, Y_train, Y_test = train_test_split(messages['text'] ,\n",
        "                                                        messages['class'],\n",
        "                                                        test_size    = 0.2,\n",
        "                                                        random_state = 0  )\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_train          = count_vectorizer.fit_transform(msg_train).toarray()\n",
        "X_test           = count_vectorizer.transform    (msg_test ).toarray()\n",
        "\n",
        "\n",
        "for model in models:\n",
        "    # ==================================\n",
        "    # TRAIN\n",
        "    # ==================================\n",
        "    start_time = timeit.default_timer()\n",
        "    model.fit(X_train, Y_train)\n",
        "    perf['train_time'].append(timeit.default_timer() - start_time)\n",
        "\n",
        "    # ==================================\n",
        "    # TEST\n",
        "    # ==================================\n",
        "    start_time = timeit.default_timer()\n",
        "    Y_pred     = model.predict(X_test)\n",
        "    perf['test_time'].append(timeit.default_timer() - start_time)\n",
        "\n",
        "    # ==================================\n",
        "    # PERFORMANCE\n",
        "    # ==================================\n",
        "    # In here, we are interrested in \"spam\" class which is our positive class\n",
        "    perf['precision'].append(precision_score(Y_test, Y_pred, pos_label='spam'))\n",
        "    perf['recall'   ].append(recall_score   (Y_test, Y_pred, pos_label='spam'))\n",
        "\n",
        "\n",
        "pd.DataFrame({\n",
        "    'Algorithm' : algos,\n",
        "    'Train time': perf['train_time'],\n",
        "    'Test time' : perf['test_time'],\n",
        "    'Precision' : perf['precision'],\n",
        "    'Recall'    : perf['recall']\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJWHv8vTm3NG"
      },
      "source": [
        "**TODO: Analyze the results**\n",
        "\n",
        "1. What do you notice about training time? (order the algorithms)\n",
        "1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to training time)\n",
        "1. What do you notice about the testing time? (order the algorithms)\n",
        "1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to testing time)\n",
        "1. Why is the Gaussian model less efficient than the multinomial based on the nature of the two algorithms?\n",
        "1. Why is the Gaussian model less efficient than the multinomial based on the nature of the problem/data?\n",
        "1. How Multinomial NB's implementation affect the training/test time? (store statistics vs. store probabilities)\n",
        "1. Which one is more adequate for updating the model with new data? explain.\n",
        "\n",
        "**Answer**\n",
        "\n",
        "1. The fastest model in terms of training time is: MNB (Multinominal Naive Bayes), followed by GNB (Gaussian Naive Bayes) in second place, and finally LR (Logistic Regression) has the slowest training time (0.551115 seconds).\n",
        "\n",
        "2. Naive Bayes algorithms (both Multinominal and Gaussian) store statistical parameters that are later used to predict the output class of a new entry. For example, they store the number of samples for each class. In the case of Multinominal Naive Bayes, for each class $k$ and feature $x_j$, the model calculates the number of instances belonging to the class $k$ that have the value $v$ for the feature $x_j$. For Gaussian Naive Bayes, we calculate $\\mu_{kj}$ (the mean of $x_j$’s values having $k$ as class) and $\\sigma^2_{kj}$ (the unbiased variance of $x_j$’s values having $k$ as class). In contrast, a logistic regression model needs to update its parameters in an iterative manner using methods such as gradient descent, which is more computantionally expensive. This explains why Naive Bayes algorithms have a faster training time compared to Logistic Regression.\n",
        "\n",
        "3. The fastest model in terms of testing time is: MNB (Multinominal Naive Bayes), followed by LR (Logistic Regression) in second place, and finally GNB (Gaussian Naive Bayes) has the slowest testing time (0.095978 seconds).\n",
        "\n",
        "4. MNB had the fastest testing time because it simply relies on word frequencies to predict the class. Logistic regression has a slightly slower testing time than MNB because it involves calculating weighted sums of all features and then applying a sigmoid function to predict the class. Although still efficient, it requires more calculations than MNB. Finally, GNB has the slowest testing time because it calculates likelihood probabilities using the normal distribution, which involves more complex calculations, resulting in a much slower testing time.\n",
        "\n",
        "5. GNB assumes a Gaussian (Normal) distribution for each feature, which doesn’t align well with discrete features in text classification as it has to estimate probabilities for a distribution that doesn’t naturally fit. MNB, on the other hand, is specifically designed for discrete data, so it operates more efficiently in this context.\n",
        "\n",
        "6. The Gaussian model is less efficient because the features represent word counts within a text, which are discrete rather than continuous. Gaussian Naive Bayes is more suitable for continuous features. Multinomial Naive Bayes, however, is designed for handling discrete data, making it a better performing model for this kind of problem.\n",
        "\n",
        "7. Store statistics: Training is faster because the model only store counts for the different classes. More precisely, for each class $k$, it records the number of instances belonging to that class. Additionally, for each class $k$, for each feature $x_j$, for each value $v$ from the possible values of $x_j$, we count the number of instances that have the value $v$ for the feature $x_j$ and belong to the class $k$. However it's slower in testing because it needs to calculate the probabilities (prior + likelihood) everytime for each sample in the test data set to make a prediction.\n",
        "\n",
        " Store Probabilities: If the model stores precalculated probabilities directly, the test time can be faster, as it avoids the need to recalculate probabilities during testing. However, training might be slightly slower as the model needs to compute and store the probabilities.\n",
        "\n",
        "\n",
        "8. The Multinominal Naive Bayes approach which stores statistics is more efficient for updating the model with new data since it only requires adjusting counts according to the new samples, which can be done in an incremental way, without having to re-do the entire calculations. In contrast, the Multinominal Naive Bayes approach which directly stores probabilities is less suitable in this case because it requires re-calculating probabilities from scratch, which is a computationally intensive task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VY0takKm3NH",
        "outputId": "2d90bef7-0050-46aa-ea35-d3e61a2f1114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  _____    __                                              _               \n",
            " |_   _|  / _|                                            | |              \n",
            "   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \n",
            "   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \n",
            "  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \n",
            " |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \n",
            "                   __/ |                     __/ |                         \n",
            "                  |___/                     |___/                          \n",
            "  _     _       _            __                                            \n",
            " | |   | |     (_)          / _|                 _                         \n",
            " | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \n",
            " | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \n",
            " | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \n",
            "  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \n",
            "                                                |/                         \n",
            "                                                                           \n",
            "                                                                           \n",
            "                                                                           \n",
            "  _   _    ___    _   _      __ _   _ __    ___                            \n",
            " | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \n",
            " | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \n",
            "  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \n",
            "   __/ |                                                                   \n",
            "  |___/                                                                    \n",
            "                    _                                                __    \n",
            "                   | |                                            _  \\ \\   \n",
            "  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \n",
            " | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \n",
            " | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \n",
            " |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \n",
            "                                                                     /_/   \n",
            "                                                                           \n"
          ]
        }
      ],
      "source": [
        "print(\"  _____    __                                              _               \")\n",
        "print(\" |_   _|  / _|                                            | |              \")\n",
        "print(\"   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \")\n",
        "print(\"   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \")\n",
        "print(\"  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \")\n",
        "print(\" |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \")\n",
        "print(\"                   __/ |                     __/ |                         \")\n",
        "print(\"                  |___/                     |___/                          \")\n",
        "print(\"  _     _       _            __                                            \")\n",
        "print(\" | |   | |     (_)          / _|                 _                         \")\n",
        "print(\" | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \")\n",
        "print(\" | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \")\n",
        "print(\" | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \")\n",
        "print(\"  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \")\n",
        "print(\"                                                |/                         \")\n",
        "print(\"                                                                           \")\n",
        "print(\"                                                                           \")\n",
        "print(\"                                                                           \")\n",
        "print(\"  _   _    ___    _   _      __ _   _ __    ___                            \")\n",
        "print(\" | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \")\n",
        "print(\" | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \")\n",
        "print(\"  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \")\n",
        "print(\"   __/ |                                                                   \")\n",
        "print(\"  |___/                                                                    \")\n",
        "print(\"                    _                                                __    \")\n",
        "print(\"                   | |                                            _  \\ \\   \")\n",
        "print(\"  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \")\n",
        "print(\" | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \")\n",
        "print(\" | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \")\n",
        "print(\" |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \")\n",
        "print(\"                                                                     /_/   \")\n",
        "print(\"                                                                           \")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}